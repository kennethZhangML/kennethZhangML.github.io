<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Chapter 3 - Brownian Motion | Kenneth Zhang </title> <meta name="author" content="Kenneth Zhang"> <meta name="description" content="Notes on Brownian motion and its applications."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kennethzhangml.github.io/notes/course-2/chapter-3-brownian-motion/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <script defer src="https://tikzjax.com/v1/tikzjax.js"></script> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Kenneth</span> Zhang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/notes/">notes <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Chapter 3 - Brownian Motion</h1> <p class="post-description">Notes on Brownian motion and its applications.</p> </header> <article> <hr> <h3 id="scaled-random-walks">Scaled Random Walks</h3> <h4 id="symmetric-random-walk">Symmetric Random Walk</h4> <p>Construct a symmetric random walk by repeatedly tossing a fair coin, (i.e., $p$ is probability of heads, and $q = 1 - p$ is probability of tails). Successive outcomes of the tosses, \(\omega = \omega_1 \omega_2 \omega_3 \dots\) $\omega$ is the infinite sequence of coin tosses and $\omega_n$ is outcome of $n$-th toss, \(X_j = \begin{cases} 1 &amp; \text{if }\omega_j = H,\\ -1 &amp; \text{if }\omega_j = T \end{cases}\) By default, $M_0 = 0$, thus, we define, \(M_k = \sum_{j = 1}^k X_j,\quad k = 1, 2, \dots\) $M_k$ is the process that is what we call a <strong>symmetric random walk</strong>. Each toss makes it steps up one unit or down one unit and each of these two possibilities is equally likely.</p> <pre><code class="language-tikz">\begin{document}
\begin{tikzpicture}[scale=2.5]
    \draw[-&gt;] (-0.5,0) -- (5.5,0) node[right] {};
    \draw[-&gt;] (0,-2.5) -- (0,1.5) node[above] {};
    \draw[very thin,gray!50] (-0.5,-2.5) grid (5.5,1.5);
    \coordinate (M0) at (0,0);
    \coordinate (M1) at (1,1);
    \coordinate (M2) at (2,0);
    \coordinate (M3) at (3,-1);
    \coordinate (M4) at (4,-2);
    \coordinate (M5) at (5,-1);
    \draw[thick] (M0) -- (M1) -- (M2) -- (M3) -- (M4) -- (M5);
    \foreach \i in {0,1,2,3,4,5} {
        \fill (M\i) circle (2pt);
    }
    \node[above left] at (M0) {$M_0$};
    \node[above] at (M1) {$M_1$};
    \node[above] at (M2) {$M_2$};
    \node[below] at (M3) {$M_3$};
    \node[below] at (M4) {$M_4$};
    \node[above] at (M5) {$M_5$};
    \foreach \x in {1,2,3,4,5} {
        \draw (\x,-0.1) -- (\x,0.1);
        \node[below] at (\x,0) {\x};
    }
    \foreach \y in {-2,-1,1} {
        \draw (-0.1,\y) -- (0.1,\y);
        \node[left] at (0,\y) {\y};
    }
\end{tikzpicture}
\end{document}
</code></pre> <hr> <p><strong>Properties of Random Walks</strong></p> <ol> <li>Independent increments, for $0 = k_0 &lt; k_1 &lt; \dots &lt; k_m$, \(M_1 = (M_{k_1} - M_{k_0}), (M_{k_2} - M_{k_1}), \dots, (M_{k_m} - M_{k_{m - 1}})\)</li> <li>Each of the random increments is a random variable that is, \(M_{k_{i + 1}} - M_{k_i} = \sum_{j = k_i + 1}^{k_{i + 1}}X_j\) which is an increment of the random walk, and is the change in position of the random walk between times $k_i$ and $k_{i + 1}$.</li> <li>Non-overlapping time intervals are independent because they depend on different coin tosses</li> <li>Each increment $M_{k_{i + 1}} - M_{k <em>i}$ has expected value 0 and variance $k</em>{i + 1} - k_i \($ Var(X_j) = \mathbb{E}X_j^2 = 1 \implies Var(M_{k_{i + 1}} - M_{k_i}) = \sum_{j = k_{i} + 1}^{k_{i + 1}} Var(X_j) = \sum_{j = k_i + 1}^{k_{i + 1}} 1 = k_{i + 1} - k_i\)</li> <li>Variance of a symmetric random walk accumulates are rate one per unit time so that variance of the increment over any time interval $k$ to $l$ for non-negative integers $k &lt; l$ is $l - k$</li> <li>A symmetric random walk is a martingale, \(\begin{align*} \mathbb{E}[M_k | \mathcal{F}_k] &amp;= \mathbb{E}[(M_l - M_k) + M_k | \mathcal{F}_k]\\ &amp;= \mathbb{E}[M_l - M_k | \mathcal{F}_k] + \mathbb{E}[M_k | \mathcal{F}_k]\\ &amp;= \mathbb{E}[M_l - M_k | \mathcal{F}_k] + M_k\\ &amp;= \mathbb{E}[M_l - M_k] + M_k = M_k \end{align*}\) Line one results from separating, line 2 results from linearity of conditional expectations, line 3 is because $M_k$ only depends on $\mathcal{F}_k$ i.e., the first $k$ coin tosses by information and conditioning, and line 4 results from independence.</li> </ol> <hr> <p><strong>Quadratic Variation of Symmetric Random Walks</strong> Quadratic variation up to time $k$ is defined as, \([M, M]_k = \sum_{j = 1}^k (M_j - M_{j - 1})^2 = k\) and is computed path-by-path, by taking the sum of all the squares of all the one-step increments $M_j - M_{j - 1}$, which are either $1$ or $-1$, thus making $\sum_{j = 1}^k 1 = k$.</p> <p><strong>Note:</strong></p> <ul> <li>$Var(M_k)$ is computed by taking the average over all paths, considering probabilities</li> <li>$[M, M]_k$ is computed along a single path, so up, down probabilities do not enter the computation</li> <li>i.e., $Var(M_k)$ is only computed theoretically whereas $[M, M]_k$ does not depend on the particular path chosen so can be computed along realized path explicitly</li> </ul> <hr> <p><strong>Scaled Symmetric Random Walk</strong> Approximate Brownian motion by speeding up time and scaling down step size of a symmetric random walk, by fixing $n \in \mathbb{Z}^+$, defining the scaled symmetric random walk as, \(W^{(n)}(t) = \frac{1}{\sqrt{n}} M_{nt}\) when $nt$ is an integer, otherwise, the above would be a linear interpolation between the values of the nearest two points (one to the left, one to the right). Like a random walk, a scaled random walk has independent increments for $0 = t_0 &lt; t_1 &lt; \dots &lt; t_m$ and $nt_j$ is an integer, we can say that: \((W^{(n)}(t_1) - W^{(n)}(t_0)), (W^{(n)}(t_2) - W^{(n)}(t_1)), \dots, (W^{(n)}(t_m) - W^{(n)}(t_{m - 1}))\) are independent increments. For $0 \le s \le t$, for $ns, nt$ are integers, we can say that: \(\mathbb{E}(W^{(n)}(t) - W^{(n)}(s)) = 0, \quad Var(W^{(n)}(t) - W^{(n)}(s)) = t - s\) since the increment is the sum of $n(t - s)$ independent random variables each with expected value 0 and variance $\frac{1}{n}$.</p> <hr> <p>e.g., We have the following increment, \(W^{(100)}(0.70) - W^{(100)}(0.20)\) i.e., $n = 100$ and $(0.70 - 0.20) \times 100 = 50$ so we have 50 independent random variables each that takes value of $\pm \frac{1}{\sqrt{100}} = \pm \frac{1}{10}$ each of which has expected value 0 and variance $\frac{1}{100}$ so variance of the increment written above is, \(50 \cdot \frac{1}{100} = 0.50 \implies \text{equals } 0.70 - 0.20\) ———————————————————————— <strong>Note:</strong> If $s, t$ are chosen so that $ns, nt$ are integers, then the first term on the RHS is independent of $\mathcal{F}(s)$ and $W^{(n)}(s)$ is $\mathcal{F}(s)$-measurable, depending only on the first $ns$ coin tosses, it proves, \(\mathbb{E}[W^{(n)}(t) | \mathcal{F}(s)] = W^{(n)}(s)\) for $0 \le s \le t$ such that $ns$ and $nt$ are integers.</p> <hr> <p><strong>Quadratic Variation of Scaled Random Walk</strong> For time $t \ge 0$, such that $nt$ is an integer, we can consider the <strong>quadratic variation of the scaled random walk,</strong> along the path, we evaluate the increment over each time step and square these increments before summing to get the <strong>length of the time interval</strong> over which we are doing the computation, $$ \begin{align*} <a href="t">W^{(n)}, W^{(n)}</a> &amp;= \sum_{j = 1}^{nt} \left[W^{(n)}\left(\frac{j}{n}\right)</p> <ul> <li>W^{(n)}\left(\frac{j - 1}{n} \right) \right]^2<br> &amp;= \sum_{j = 1}^nt \left[\frac{1}{\sqrt{n}} X_j \right]^2 = \sum_{j = 1}^{nt} \frac{1}{n} = t \end{align*} $$ NOT an average over all possible paths, but obtains the same answer $t$ along all paths.</li> </ul> <p><strong>Limiting Distribution of Scaled Random Walk</strong> Fix time $t$, consider set of all possible paths at the time $t$ for a scaled random walk, and think about the scaled random walk corresponding to different values of $\omega$, sequence of tosses.</p> <p>e.g., $t = 0.25$ and consider set of all possible values of $W^{(100)}(0.25) = \frac{1}{10} M_{25}$. Thus, $n = 100$ and we have $100 \times 0.25 = 25$ coin tosses, because $M_{25}$ can take any value of odd integer between $-25$ and $25$. So $W^{(100)}(0.25)$ can take any value: \(-2.5, -2.3, -2.1, \dots, -0.1, 0.1, \dots, 2.1, 2.3, 2.5\) For $W^{(100)}(0.25) = 0.1$ that is to equal 0.1, we must get 13 heads and 12 tails in 25 tosses, \(\mathbb{P}\{W^{(100)} (0.25) = 0.1\} = \frac{25!}{13!12!} \left(\frac{1}{2}\right)^{25} = 0.1555\) From a plot, we can see that this information nearly is normal, with expected value zero and variance 0.25. Thus, we can get a good approximation for the function $g(x)$ that is continuous and models this distribution by multiplying $g(x)$ by the normal density and integrating, \(\mathbb{E}g(W^{(100)}(0.25)) \approx = \frac{2}{2 \pi} \int_{-\infty}^\infty g(x) e^{-2x^2}dx\) ————————————————————————</p> <p><strong>Theorem</strong>: Central Limit $t \ge 0$, and as $n \rightarrow \infty$, the distribution of the scaled random walk $W^{(n)}(t)$ evaluated at $t$ converges to the normal distribution with mean zero and variance $t$.</p> <p><strong>Proof:</strong> \(\begin{align} f(x) &amp;= \frac{1}{\sqrt{2\pi t}} e^{-\frac{x^2}{2t}} \\ \varphi(u) &amp;= \int_{-\infty}^{\infty} e^{ux} f(x) dx \\ &amp;= \frac{1}{\sqrt{2\pi t}} \int_{-\infty}^{\infty} \exp \left\{ ux - \frac{x^2}{2t} \right\} dx \\ &amp;= e^{\frac{1}{2} u^2 t} \cdot \frac{1}{\sqrt{2\pi t}} \int_{-\infty}^{\infty} \exp \left\{ - \frac{(x - ut)^2}{2t} \right\} dx \\ &amp;= e^{\frac{1}{2} u^2 t} \\ \varphi_n(u) &amp;= \mathbb{E} e^{u W^{(n)}(t)} = \mathbb{E} \exp \left\{ \frac{u}{\sqrt{n}} M_{nt} \right\} \\ &amp;= \mathbb{E} \exp \left\{ \frac{u}{\sqrt{n}} \sum_{j=1}^{nt} X_j \right\} = \mathbb{E} \prod_{j=1}^{nt} \exp \left\{ \frac{u}{\sqrt{n}} X_j \right\} \\ &amp;= \prod_{j=1}^{nt} \mathbb{E} \exp \left\{ \frac{u}{\sqrt{n}} X_j \right\} \\ &amp;= \prod_{j=1}^{nt} \left( \frac{1}{2} e^{\frac{u}{\sqrt{n}}} + \frac{1}{2} e^{-\frac{u}{\sqrt{n}}} \right) \\ &amp;= \left( \frac{1}{2} e^{\frac{u}{\sqrt{n}}} + \frac{1}{2} e^{-\frac{u}{\sqrt{n}}} \right)^{nt} \\ \log \varphi_n(u) &amp;= nt \log \left( \frac{1}{2} e^{\frac{u}{\sqrt{n}}} + \frac{1}{2} e^{-\frac{u}{\sqrt{n}}} \right) \\ \lim_{n \to \infty} \log \varphi_n(u) &amp;= t \lim_{x \to 0} \frac{\log \left( \frac{1}{2} e^{ux} + \frac{1}{2} e^{-ux} \right)}{x^2} \\ \frac{\partial}{\partial x} \log \left( \frac{1}{2} e^{ux} + \frac{1}{2} e^{-ux} \right) &amp;= \frac{\frac{u}{2} e^{ux} - \frac{u}{2} e^{-ux}}{\frac{1}{2} e^{ux} + \frac{1}{2} e^{-ux}} \\ \frac{\partial}{\partial x} x^2 &amp;= 2x \\ \lim_{n \to \infty} \log \varphi_n(u) &amp;= t \lim_{x \to 0} \frac{\frac{u}{2} e^{ux} - \frac{u}{2} e^{-ux}}{2x \left( \frac{1}{2} e^{ux} + \frac{1}{2} e^{-ux} \right)} \\ \lim_{x \to 0} \left( \frac{1}{2} e^{ux} + \frac{1}{2} e^{-ux} \right) &amp;= 1 \\ \frac{\partial}{\partial x} \left( \frac{u}{2} e^{ux} - \frac{u}{2} e^{-ux} \right) &amp;= \frac{u^2}{2} e^{ux} + \frac{u^2}{2} e^{-ux} \\ \frac{\partial}{\partial x} x &amp;= 1 \\ \lim_{n \to \infty} \log \varphi_n(u) &amp;= \frac{t}{2} \lim_{x \to 0} \left( \frac{\frac{u^2}{2} e^{ux} + \frac{u^2}{2} e^{-ux}}{1} \right) \\ &amp;= \frac{1}{2} u^2 t \end{align}\) ————————————————————————</p> <p><strong>Log-Normal Distribution as Limit of Binomial Model</strong> Build a model for a stock price on the time interval $[0, t]$, by choosing $n \in \mathbb{Z}$ and creating a binomial for stock price that takes $n$ steps per unit time. \(u_n = 1 + \frac{\sigma}{\sqrt{n}},\quad, d_n = 1 - \frac{\sigma}{\sqrt{n}}\) where $u_n$ and $d_n$ are the up and down factor, respectively. $\sigma$ is a positive constant, which is also the volatility of the limiting stock price process. \(\tilde{p} = \frac{1 + r - d_n}{u_n - d_n} = \frac{\sigma / \sqrt{n}}{2 \sigma / \sqrt{n}} = \frac{1}{2},\quad \tilde{q} = \frac{u_n - 1 - r}{u_n - d_n} = \frac{\sigma / \sqrt{n}}{2 \sigma / \sqrt{n}} = \frac{1}{2}\) $nt$ is the sum of $H_{nt}$ and $T_{nt}$ the number of heads and tails in the first $nt$ coin tosses, respectively. We can write the following, given random walk $M_{nt}$, \(nt = H_{nt} + T_{nt} \implies M_{nt} = H_{nt} - T_{nt} \implies H_{nt} = \frac{1}{2}(nt + M_{nt})\) the same can be done with the opposite sign for $T_{nt}$. For the model with $u_n$ and $d_n$ the stock price at $t$ is, \(S_n(t) = S(0) u_n^{H_{nt}} d_n^{T_{nt}} = S(0) \left(1 + \frac{\sigma}{\sqrt{n}} \right)^{\frac{1}{2} (nt + M_{nt})} \cdot \left(1 - \frac{\sigma}{\sqrt{n}} \right)^{\frac{1}{2} (nt - M_{nt})}\) <strong>Theorem</strong>: $n \rightarrow \infty$ the distribution of $S_n(t)$, \(S_n(t) = S(0) u_n^{H_{nt}} d_n^{T_{nt}} = S(0) \left(1 + \frac{\sigma}{\sqrt{n}} \right)^{\frac{1}{2} (nt + M_{nt})} \cdot \left(1 - \frac{\sigma}{\sqrt{n}} \right)^{\frac{1}{2} (nt - M_{nt})}\) converges to the distribution of, \(S(t) = S(0) \exp \left\{\sigma W(t) - \frac{1}{2} \sigma^2 t \right\}\) where $W(t)$ is a normal random variable with mean 0 and variance $t$, $S(t)$ is log-normal. Generally, $ce^X$ for any random variable $X$ is normally distributed. For the above, $X = \sigma W(t) - \frac{1}{2} \sigma^2 t$ with mean $-\frac{1}{2} \sigma^2 t$ and variance $\sigma^2 t$.</p> <p>The proof follows by using the Taylor expansion of $f(x) = \log (1 + x)$. \(\begin{align*} \log S_n(t) &amp;= \log S(0) + \frac{1}{2} (nt + M_{nt}) \log \left(1 + \frac{\sigma}{\sqrt{n}} \right) + \frac{1}{2}(nt - M_{nt}) \log \left(1 - \frac{\sigma}{\sqrt{n}} \right)\\ &amp;= \log S(0) + \frac{1}{2} (nt + M_{nt}) \left(\frac{\sigma}{\sqrt{n}} - \frac{\sigma^2}{2n} + O\left(n^{-\frac{3}{2}}\right) \right) \\ &amp;+ \frac{1}{2} (nt - M_{nt}) \left(-\frac{\sigma}{\sqrt{n}} - \frac{\sigma^2}{2n} + O\left(n^{-\frac{3}{2}} \right) \right)\\ &amp;= \log S(0) + nt \left(- \frac{\sigma^2}{2n} + O \left(n^{-\frac{3}{2}} \right) \right) + M_{nt} \left(\frac{\sigma}{\sqrt{n}} + O \left(n^{-\frac{3}{2}} \right) \right)\\ &amp;= \log S(0) - \frac{1}{2} \sigma^2 t + O\left(n^{-\frac{1}{2}} \right) + \sigma W^{(n)}(t) + O(n^{-1}) W^{(n)}(t) \end{align*}\) The distribution of $W^{(n)}(t) = \frac{1}{\sqrt{n}} M_{nt}$ converges to the distribution of a normal random variable with mean zero and variance $t$ a random variable we call $W(t)$. $W^{(n)}(t)$ is multiplied by a term that has $n$ in the denominator and this will have limit zero.</p> <hr> <p><strong>Brownian Motion</strong> Take the limit of scaled random walks $W^{(n)}(t)$ as $n \rightarrow \infty$. From this we can get the Brownian motion which inherits properties from the scaled random walk,</p> <p><strong>Definition: Brownian Motion</strong> For a probability space, for each $\omega \in \Omega$, suppose there is a continuous function $W(t)$ of $t \ge 0$ that satisfies $W(0) = 0$ and depends on $\omega$. Then $W(t)$ for $t \ge 0$ is a Brownian motion if for all $0 = t_0 &lt; t_1 &lt; \dots &lt; t_m$ the increments, \(W(t_1) = W(t_1) - W(t_0), W(t_2) - W(t_1), \dots, W(t_m) - W(t_{m - 1})\) are independent and each of these increments is normally distributed and has expected value 0 and variance $t_{i + 1} - t_i$.</p> <ul> <li>Brownian motion, unlike a scaled random walk, has no linear pieces</li> <li>Also, BMs are exactly normal for each $t$ as a consequence of the Central Limit theorem.</li> <li>Increments $W(t) - W(s)$ is normally distributed for all $0 \le s &lt; t$.</li> </ul> <p><strong>Intuition</strong></p> <ol> <li>A random experiment is performed and the outcome is the path of the BM, so $W(t)$ is the value of the path at time $t$ and this value depends on which path resulted from the random experiment</li> <li>$\omega$ is akin to the outcome of a sequence of coin tosses although now the coin is being toss “infinitely fast”; once the coin tosses are performed and the result $\omega$ is being obtained, then the path of the BM can be drawn. If tossed again, a different $\omega$ is obtained and a different path will be drawn For $\mathcal{F}$ is the $\sigma$-algebra of subsets of $\Omega$ whose probabilities are defined, $\mathbb{P}$ is the probability measure for which distributional statements are made about the Brownian Motion.</li> </ol> <hr> <p>e.g., let ${\omega: 0 \le W^{(100)} (0.25) \le 0.2}$ be the set we are working in, Define $M_{25} = 10 W^{(100)} (0.25)$ must fall between 0 and 2 after 25 tosses. because \(0 \times 0 \le M_{25} \le 10 \times 0.2 \implies 0 \le 10 W^{(100)}(0.25) \le 2\) Since $M_{25}$ can only be an odd number, it falls between 0 and 2, it can only be equal to 1, \(W^{(100)}(0.25) = 0.1\) Thus, we must get 13 heads and 12 tails in the first 25 tosses. Thus, we can describe the probability of this set as the following, \(\mathbb{P}\{0 \le W(0.25) \le 0.2\} = \frac{2}{\sqrt{2 \pi}} \int_0^{0.2} e^{-2x^2}dx\) ———————————————————————— <strong>Distribution of Brownian Motion</strong> As we said before, the increments of a Brownian motions are independent and normally distributed. Thus, the Brownian motion at specific time-steps are jointly normally distributed.</p> <p>Since each $W(t_i)$ has mean 0, then the covariance for $0 \le s &lt; t$, $W(s), W(t)$ is, \(\begin{align*} \mathbb{E}[W(s)W(t)] &amp;= \mathbb{E}[W(s)(W(t) - W(s)) + W^2(s)]\\ &amp;= \mathbb{E}[W(s)] \cdot \mathbb{E}[W(t) - W(s)] + \mathbb{E}[W^2(s)]\\ &amp;= 0 + Var[W(s)] = s, \end{align*}\) We define the <strong>$m$-dimensional covariance matrix</strong> for the brownian motion, \(\begin{bmatrix} \mathbb{E} \left[ W^2(t_1) \right] &amp; \mathbb{E} \left[ W(t_1) W(t_2) \right] &amp; \cdots &amp; \mathbb{E} \left[ W(t_1) W(t_m) \right] \\ \mathbb{E} \left[ W(t_2) W(t_1) \right] &amp; \mathbb{E} \left[ W^2(t_2) \right] &amp; \cdots &amp; \mathbb{E} \left[ W(t_2) W(t_m) \right] \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mathbb{E} \left[ W(t_m) W(t_1) \right] &amp; \mathbb{E} \left[ W(t_m) W(t_2) \right] &amp; \cdots &amp; \mathbb{E} \left[ W^2(t_m) \right] \end{bmatrix} = \begin{bmatrix} t_1 &amp; t_1 &amp; \cdots &amp; t_1 \\ t_1 &amp; t_2 &amp; \cdots &amp; t_2 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ t_1 &amp; t_2 &amp; \cdots &amp; t_m \end{bmatrix}\) To get a moment-generating function for a zero-mean normal random variable with variance $t$ and the independence of the increments as specified previously, \(\begin{align} u_3 W(t_3) + u_2 W(t_2) + u_1 W(t_1) &amp;= u_3 (W(t_3) - W(t_2)) + (u_2 + u_3)(W(t_2) - W(t_1)) \notag \\ &amp;\quad + (u_1 + u_2 + u_3)W(t_1) \sum_{j=1}^{m} u_j W(t_j) \\ &amp;= u_m (W(t_m) - W(t_{m-1})) + (u_{m-1} + u_m)(W(t_{m-1}) - W(t_{m-2})) \notag \\ &amp;\quad + \dots + (u_1 + \dots + u_m) W(t_1) \varphi(u_1, \dots, u_m) \\ &amp;= \mathbb{E} \exp \left\{ u_m W(t_m) + u_{m-1} W(t_{m-1}) + \dots + u_1 W(t_1) \right\} \\ &amp;= \exp \Bigg\{ \frac{1}{2} (u_1 + \dots + u_m)^2 t_1 + \frac{1}{2} (u_2 + \dots + u_m)^2 (t_2 - t_1) \notag \\ &amp;\quad + \dots + \frac{1}{2} (u_{m-1} + u_m)^2 (t_{m-1} - t_{m-2}) + \frac{1}{2} u_m^2 (t_m - t_{m-1}) \Bigg\} \end{align}\) Thus, the moment-generating function for a Brownian motion (i.e., the $m$-dimensional random vector $(W(t_1), W(t_2), \dots, W(t_m))$ is given by, \(\phi(x) = \exp \Bigg\{ \frac{1}{2} (u_1 + \dots + u_m)^2 t_1 + \frac{1}{2} (u_2 + \dots + u_m)^2 (t_2 - t_1) + \dots + \frac{1}{2} (u_{m-1} + u_m)^2 (t_{m-1} - t_{m-2}) + \frac{1}{2} u_m^2 (t_m - t_{m-1}) \Bigg\}\) The distribution of the brownian increments can be specified by the specifying the joint-density or the joint-moment-generating function of the random variables $W(t_1), W(t_2), \dots, W(t_n)$.</p> <p><strong>Theorem</strong>: Characterizations of Brownian Motions We have a continuous function $W(t)$ for $W(0) = 0$, that depends on $\omega$, thus the following three properties are considered equivalent,</p> <ol> <li>For all $0 = t_0 &lt; t_1 &lt; \dots &lt; t_m$, the increments, \(W(t_1) = W(t_1) - W(t_0), W(t_2) - W(t_1), \dots, W(t_m) - W(t_{m - 1})\) are independent and each is normally distributed with mean $0$ and variance $t_i - t_{i + 1}$</li> <li>For all $0 = t_0 &lt; t_1 &lt; \dots &lt; t_m$ the random variables $W(t_1), W(t_2), \dots, W(t_m)$ are jointly normally distributed with means equal to 0 and covariance matrix given previously</li> <li>For all “”, the random variables “”, have jointly moment-generating function, $$ \phi(x) = \exp \Bigg{ \frac{1}{2} (u_1 + \dots + u_m)^2 t_1 <ul> <li>\frac{1}{2} (u_2 + \dots + u_m)^2 (t_2 - t_1) + \dots + \frac{1}{2} (u_{m-1} + u_m)^2 (t_{m-1} - t_{m-2})</li> <li>\frac{1}{2} u_m^2 (t_m - t_{m-1}) \Bigg} $$ ————————————————————————</li> </ul> </li> </ol> <p><strong>Filtration for Brownian Motion</strong> For a filtration $\mathcal{F}(t)$ for the Brownian Motion is a collection of $\sigma$-algebras $\mathcal{F}(t)$ satisfying,</p> <ol> <li>Every set in $\mathcal{F}(s)$ is also in $\mathcal{F}(t)$ for $s &lt; t$, thus there is at least as much information available at time later time $\mathcal{F}(t)$ as there is at the earlier time $\mathcal{F}(s)$.</li> <li>Information available at time $t$ is sufficient to evaluate the Brownian motion at that time; for each $t \ge 0$, the BM $W(t)$ is $\mathcal{F}(t)$-measurable</li> <li>The increment $W(u) - W(t)$ is independent of $\mathcal{F}(t)$, any increment of the BM after $t$ is independent of the information available at time $t$ $\Delta(t)$ is a stochastic process, and is adapted to $\mathcal{F}(t)$ if for each $t \ge 0$ the r.v. $\Delta(t)$ is $\mathcal{F}(t)$-measurable.</li> </ol> <p><strong>Two Possibilities of $\mathcal{F}(t)$ for BM</strong></p> <ol> <li>contains only info obtained by observing BM up to $t$</li> <li>contains info obtained by observing BM and one or more other processes</li> </ol> <p><strong>Martingale Property for Brownian Motion</strong> Brownian motion is a martingale.</p> <p><strong>Proof:</strong> For $0 \le s &lt; t$ then we have the following, \(\begin{align*} \mathbb{E}[W(t) | \mathcal{F}(s)] &amp;= \mathbb{E}[W(t) - W(s) + W(s) | \mathcal{F}(s)]\\ &amp;= \mathbb{E}[W(t) - W(s) | \mathcal{F}(s)] + \mathbb{E}[W(s) | \mathcal{F}(s)]\\ &amp;= \mathbb{E}[W(t) - W(s) ] + W(s)\\ &amp;= W(s) \end{align*}\) ————————————————————————</p> <p><strong>Quadratic Variation</strong></p> <p>We showed that the quadratic variation of a scaled random walk $W^{(n)}(t)$ up to time $T$ is $T$. For a BM, there is no natural step size, so if we are given $T &gt; 0$ then we could choose a step size $\frac{T}{n}$ for some large $n$, and compute the quadratic variation up to time $T$ with this step size. \(\sum_{j = 0}^{n - 1} \left[W \left(\frac{(j + 1)T}{n} \right) - W\left(\frac{jT}{n} \right) \right]^2\) We want to evaluate this quantity but for small step sizes, so we take the limit of the above from $n \rightarrow \infty$. Thus, we also get $T$ as the quadratic variation. Paths of BM are unusual in that their quadratic variation is not zero making stochastic calculus different from ordinary.</p> <hr> <p><strong>First Order Variation</strong> <strong>Goal</strong>: Compute amount of up and down oscillation undergone by $f(t)$ between $[0, T]$, with down moves adding and up moves subtracting (counterintuitive, but this will be useful in the future).</p> <p>For $f$, \(\begin{align*} FV_T(f) &amp;= [f(t_1) - f(0)] - [f(t_2) - f(t_1)] + [f(T) - f(t_2)]\\ &amp;= \int_0^{t_1} f'(t) dt + \int_{t_1}^{t_2} (-f'(t))dt + \int_{t_2}^T f'(t) dt\\ &amp;= \int_0^T |f'(t)| dt \end{align*}\) where the middle term $-[f(t_2) - f(t_1)] = f(t_1) - f(t_2)$ guarantees that the magnitude of the down move of the function $f(t)$ between $t_1, t_2$ is added to rather than subtracted from total.</p> <p>Take partition $\Pi = {t_0, t_1, \dots t_n}$ of $[0, T]$ which is a set of times, not necessarily equally spaces: \(0 = t_0 &lt; t_1 &lt; \dots &lt; t_n = T\) with max step size of the partition denoted: \(||\Pi|| = \max_{j = 0, \dots, n - 1}(t_{j + 1} - t_j)\) Thus, first order variation, we take the limit as the number of $n$ partition points goes to infinity, and the longest subinterval $t_{j + 1} - t_j$ goes to zero. \(FV_T(f) = \lim_{||\Pi|| \rightarrow 0} \sum_{j = 1}^{n - 1} | f(t_{j + 1}) - f(t_j)|\) Using MVT, we can show, \(\frac{f(t_{j + 1})-f(t_j)}{t_{j + 1}-t_j} = f'(t_j^*)\) on interval $[t_j, t_{j + 1}]$ there is such a point $t_j^*$. Thus, somewhere between these two points, the tangent line is parallel to the chord. Thus, we can get: \(f(t_{j + 1}) - f(t_j) = f'(t_j^*)(t_{j + 1} - t_j) \implies \sum_{j = 1}^{n - 1} | f'(t_j^*)| (t_{j + 1} - t_j)\) which is the Riemann sum for the integral of $|f’(t)|$, thus we get \(FV_T(f) = \lim_{||\Pi|| \rightarrow 0}\sum_{j = 1}^{n - 1} |f'(t_j^*)|(t_{j + 1} - t_j) = \int_0^T |f'(t)|dt\) ————————————————————————</p> <p><strong>Definition: Quadratic Variation</strong> The quadratic variation of $f$ up to time $T$ is given by, \([f, f](T) = \lim_{||\Pi|| \rightarrow 0} \sum_{j = 0}^{n - 1} [f(t_{j + 1} - f(t_j))]^2\) where $\Pi$ and the partitions are defined like before.</p> <p>Suppose $f$ has a continuous derivative, then \(\sum_{j = 0}^{n - 1} [f(t_{j + 1}) - f(t_j)]^2 = \sum_{j = 0}^{n - 1} |f'(t_j^*)|^2 (t_{j + 1} - t_j)^2 \le ||\Pi| \cdot \sum_{j = 0}^{n - 1} |f'(t_j^*)|^2 (t_{j + 1} - t_j)\)</p> <p>\(\begin{align} [f,f](T) &amp;= \lim_{\|\Pi\| \to 0} \left[ \|\Pi\| \cdot \sum_{j=0}^{n-1} |f'(t_j^*)|^2 (t_{j+1} - t_j) \right] \\ &amp;= \lim_{\|\Pi\| \to 0} \|\Pi\| \cdot \lim_{\|\Pi\| \to 0} \sum_{j=0}^{n-1} |f'(t_j^*)|^2 (t_{j+1} - t_j) \\ &amp;= \lim_{\|\Pi\| \to 0} \int_0^T |f'(t)|^2 dt = 0. \end{align}\)</p> <ul> <li>The quadratic variation measures the accumulation of squared increments over finer and finer partitions of an interval</li> <li>If $f$ is continuously differentiable then its quadratic variation disappears, meaning it does not exhibit erratic or jumpy behaviour</li> <li>Thus, as we can see from the derivation above, the quadratic variation for a continuous-derivative function, is <strong>zero</strong>.</li> <li>Thus, we never consider quadratic variation in ordinary calculus;</li> <li>paths of brownian motion can not be differentiated with respect to the time variable</li> </ul> <hr> <p><strong>Theorem:</strong> $W$ is a Brownian Motion, then $<a href="T">W, W</a> = T$ for all $T \ge 0$ almost surely.</p> <p><strong>Proof:</strong> For sampled quadratic variation corresponding to this partition, we have, \(Q_{\Pi} = \sum_{j = 0}^{n - 1} (W(t_{j + 1}) - W(t_j))^2\) We show that this is a random variable and converges to $T$ as $\left\lVert \Pi\right\rVert \rightarrow 0$. Moreover, it has expected value $T$ and variance that converges to 0. Hence, it converges to expected value $T$ regardless of the path along which we are doing the computation.</p> <hr> <p>The sampled quadratic variation is the sum of independent random variables, therefore its mean and variance are the sums of the means and variances of these random variables. \(\mathbb{E} \left[(W(t_{j + 1}) - W(t_j))^2 \right] = Var\left[W(t_{j + 1}) - W(t_j) \right] = t_{j + 1} - t_j\) Which gives us the implication that, \(\mathbb{E} Q_{\Pi} = \sum_{j = 0}^{n - 1} \mathbb{E}\left[(W(t_{j + 1}) - W(t_j))^2 \right] = \sum_{j = 0}^{n - 1} (t_{j + 1} - t_j) = T\) Moreover, \(\begin{align} \text{Var} \left[ (W(t_{j+1}) - W(t_j))^2 \right] &amp;= \mathbb{E} \left[ \left( (W(t_{j+1}) - W(t_j))^2 - (t_{j+1} - t_j) \right)^2 \right] \\ &amp;= \mathbb{E} \left[ (W(t_{j+1}) - W(t_j))^4 \right] - 2 (t_{j+1} - t_j) \mathbb{E} \left[ (W(t_{j+1}) - W(t_j))^2 \right] \\&amp;+ (t_{j+1} - t_j)^2. \end{align}\)</p> \[\begin{align} \mathbb{E} \left[ (W(t_{j+1}) - W(t_j))^4 \right] &amp;= 3 (t_{j+1} - t_j)^2. \end{align}\] \[\begin{align} \text{Var} \left[ (W(t_{j+1}) - W(t_j))^2 \right] &amp;= 3 (t_{j+1} - t_j)^2 - 2 (t_{j+1} - t_j)^2 + (t_{j+1} - t_j)^2 \\ &amp;= 2 (t_{j+1} - t_j)^2. \end{align}\] \[\begin{align} \text{Var}(Q_{\Pi}) &amp;= \sum_{j=0}^{n-1} \text{Var} \left[ (W(t_{j+1}) - W(t_j))^2 \right] \\ &amp;= \sum_{j=0}^{n-1} 2 (t_{j+1} - t_j)^2 \\ &amp;\leq \sum_{j=0}^{n-1} 2 \| \Pi \| (t_{j+1} - t_j) \\ &amp;= 2 \| \Pi \| T. \end{align}\] <p>\(\begin{align} \lim_{\|\Pi\| \to 0} \text{Var}(Q_{\Pi}) &amp;= 0, \quad \text{and we conclude that} \quad \lim_{\|\Pi\| \to 0} Q_{\Pi} = \mathbb{E} Q_{\Pi} = T. \end{align}\) Above, we saw that $\mathbb{E}[(W(t_{j + 1}) - W(t_j))^2] = t_{j + 1} - t_j$ and $Var[(W(t_{j + 1}) - W(t_j))^2] = 2(t_{j + 1} - t_j)^2$ so when $t_{j + 1} - t_j$ is small, the square of it is very small. Therefore, we can write, \((W(t_{j + 1}) - W(t_j))^2 \approx t_{j + 1} - t_j\) Probabilistically, given the approximate equality above, we can write, \(\frac{(W(t_{j + 1}) - W(t_j))^2}{t_{j + 1} - t_j} \approx 1\) but is not in fact near one, no matter how small $t_{j + 1} - t_j$ is, so it is the square of the standard normal random variable given by, \(Y_{j + 1} = \frac{W(t_{j + 1}) - W(t_j)}{\sqrt{t_{j + 1} - t_j}}\) and its distribution is the same no matter how small we make $t_{j + 1} - t_j$. If we choose large $n$ and $t_j = \frac{jT}{n}$ then for $t_{j + 1} - t_j = \frac{T}{n}$ for all $j$, and \((W(t_{j + 1}) - W(t_j))^2 = T \cdot \frac{Y_{j + 1}^2}{n}\) Since $Y_1, Y_2, \dots, Y_n$ are iid, then by SLLN, we can see that, \(\sum_{j = 0}^{n - 1} \frac{Y_{j + 1}^2}{n} \rightarrow \mathbb{E}Y_{j + 1}^2 \text{ as } n \rightarrow \infty\) thus, we can say that, \(\sum_{j = 0}^{n - 1}(W(t_{j + 1}) - W(t_j))^2 \rightarrow T\) Thus we can write that, \(dW(t)dW(t) = dt\) meaning that on an interval $[0, T]$ the BM accumulates $T$ units of quadratic variation. Additionally, let’s take $0 &lt; T_1 &lt; T_2$, then on an interval $[T_1, T_2]$ the sum of the squared increments of BM for each of the subintervals in the partition gives: \([W, W](T_2) - [W, W](T_1) = T_2 - T_1\) Thus, the BM accumulates $T_2 - T_1$ units of QV over the interval $[T_1, T_2]$, and more generally, we can say that BM accumulates quadratic variation at rate one per unit time.</p> <hr> <p><strong>Cross Variation</strong> We can compute the cross variation of $W(t)$ with $t$ and the QV of $t$ with itself, \(\lim_{||\Pi|| \rightarrow 0} \sum_{j = 0}^{n - 1} (W(t_{j + 1}) - W(t_j))^2 = T \implies \lim_{||\Pi|| \rightarrow 0} \sum_{j= 0 }^{n - 1} (W(t_{j + 1}) - W(t_j))(t_{j + 1} - t_j) = 0\)</p> <p>\(\implies \lim_{||\Pi|| \rightarrow 0} \sum_{j = 0}^{n - 1}(t_{j + 1} - t_j)^2 = 0\) In the first line, we can see that the limit is 0 because, \(\begin{align} \left| (W(t_{j+1}) - W(t_j))(t_{j+1} - t_j) \right| &amp;\leq \max_{0 \leq k \leq n-1} |W(t_{k+1}) - W(t_k)| (t_{j+1} - t_j), \\ \left| \sum_{j=0}^{n-1} (W(t_{j+1}) - W(t_j))(t_{j+1} - t_j) \right| &amp;\leq \max_{0 \leq k \leq n-1} |W(t_{k+1}) - W(t_k)| \cdot T. \end{align}\)</p> <p>\(\begin{align} \sum_{j=0}^{n-1} (t_{j+1} - t_j)^2 &amp;\leq \max_{0 \leq k \leq n-1} (t_{k+1} - t_k) \cdot \sum_{j=0}^{n-1} (t_{j+1} - t_j) \\ &amp;= \|\Pi\| \cdot T. \end{align}\) Thus, we can capture the essence of this by writing, \(\begin{align} dW(t) dt &amp;= 0, \quad dtdt = 0. \quad \quad (3.4.14) \end{align}\) ————————————————————————</p> <p><strong>Volatility of Geometric Brownian Motion</strong> For constants $\alpha, \sigma &gt; 0$, we can define the Geometric Brownian Motion, \(S(t) = S(0) \exp \left\{\sigma W(t) + \left(\alpha - \frac{1}{2}\sigma^2 \right) \right\}\) i.e., the asset-price model used in the Black-Scholes-Merton option-pricing formula. We show how to use the quadratic variation of BM to identify the volatility $\sigma$ from a path of this process.</p> <p>Using $0 \le T_1 &lt; T_2$ and defined $T_1 = t_0 &lt; t_2 &lt; \dots &lt; t_m = T_2$ and observe the <strong>log returns</strong>, \(\log \frac{S(t_{j + 1})}{S(t_j)} = \sigma (W(t_{j + 1}) - W(t_j)) + \left(\alpha - \frac{1}{2}\sigma^2 \right)(t_{j + 1} - t_j)\) over each subinterval $[t_j, t_{j + 1}]$. The <strong>sum of squares</strong> of the <strong>log returns</strong> is also called the <strong>realized volatility</strong> and is given by, $$ \begin{align} \sum_{j=0}^{m-1} \left( \log \frac{S(t_{j+1})}{S(t_j)} \right)^2 &amp;= \sigma^2 \sum_{j=0}^{m-1} \left( W(t_{j+1}) - W(t_j) \right)^2</p> <ul> <li>\left( \alpha - \frac{1}{2} \sigma^2 \right) \sum_{j=0}^{m-1} (t_{j+1} - t_j)^2 <br> &amp;\quad + 2\sigma \left( \alpha - \frac{1}{2} \sigma^2 \right) \sum_{j=0}^{m-1} (W(t_{j+1}) - W(t_j))(t_{j+1} - t_j). \end{align} $$ <ol> <li>Term 1: is approximately equal to its limit which is $\sigma^2$ times the amount of QV accumulated by Brownian Motion on the interval $[T_1, T_2]$ which is $T_2 - T_1$.</li> <li>Term 2: is the $\left(\alpha - \frac{1}{2}\sigma^2\right)^2$ times the QV of $t$ which is 0</li> <li>Term 3: is $2 \sigma \left(\alpha - \frac{1}{2}\sigma^2 \right)$ times the cross variation of $W(t)$ and $t$ which is zero</li> </ol> </li> </ul> <p>When the maximum step size is small, then the RHS of the above is approx. equal to $\sigma^2(T_2 - T_1)$. \(\frac{1}{T_2 - T_1} \sum_{j = 0}^{m - 1}\left(\log \frac{S(t_{j + 1})}{S(t_j)} \right)^2 \approx \sigma^2\) If $S(t)$ is a GBM with constant volatility $\sigma$ then $\sigma$ is identified from price observations by computing the LHS of the above and taking the root.</p> <hr> <p><strong>Markov Property</strong> Let $W(t)$ be a Brownian motion and let $\mathcal{F}(t)$ be a filtration for the Brownian motion, then $W(t)$ is a Markov process.</p> <p><strong>Proof</strong> $0 \le s \le t$ and let $f$ be a Borel-measurable function, then we define another Borel-measurable function $g$ such that, \(\mathbb{E}[f(W(t)) | \mathcal{F}(s)] = \mathbb{E}[f((W(t) - W(s))) + W(s) | \mathcal{F}(s)] \mathbb{E}[f(W(t)) | \mathcal{F}(s)] = g(W(s))\) where $W(t) - W(s)$ is independent of $\mathcal{F}(s)$ and the random variable $W(s)$ is $\mathcal{F}(s)$-measurable. Thus we compute the expectation on the RHS by replacing $W(s)$ with $x$ and hold it constant and take the unconditional expectation of the remaining random variable. \(g(x) = \mathbb{E}f(W(t) - W(s) + x)\) $W(t) - W(s)$ is normally distributed with mean 0 and variance $t - s$ meaning we can write, \(g(x) = \frac{1}{\sqrt{2\pi (t-s)}} \int_{-\infty}^\infty f(w + x) e^{- \frac{w^2}{2(t-s)}}dw\) Thus, we can now take $g(x)$ and replace $x$ with r.v. $W(s)$, let $\tau = t - s$ and $y = w + x$, \(g(x) = \frac{1}{\sqrt{2 \pi \tau}} \int_{-\infty}^{\infty} f(y) e^{-\frac{(y - x)^2}{2\tau}} dy\) Thus, we define the <strong>transition density for Brownian Motion</strong> $p(\tau, x, y)$ to be, \(p(\tau, x, y) = \frac{1}{\sqrt{2 \pi \tau}} \exp\left\{-\frac{(y-x)^2}{2\tau} \right\}\) so we can decompose and re-arrange this into, \(g(x) = \int_{-\infty}^{\infty} f(y) p(\tau, x, y) dy \implies \mathbb{E}[f(W(t)) | \mathcal{F}(s)] = \int_{-\infty}^{\infty} f(y) p (\tau, W(s), y) dy\)</p> <ul> <li>Conditioned on the information in $\mathcal{F}(s)$, the conditional density of $W(t)$ is $p(\tau, W(s), y)$</li> <li>aka, the density in variable $y$</li> <li>Density is normal with mean $W(s)$ and variance $\tau = t - s$.</li> <li>Information from $\mathcal{F}(s)$ is the only info relevant to value of $W(s)$</li> <li>$W(s)$ is relevant is essence of the Markov property</li> </ul> <hr> <p><strong>Theorem: Exponential Martingale</strong> Exponential martingale corresponding to $\sigma$, is given by \(Z(t) = \exp \left\{\sigma W(t) - \frac{1}{2} \sigma^2 t \right\}\) where $W(t)$ is a BM with filtration $\mathcal{F}(t)$, the process $Z(t)$ is a martingale.</p> <p><strong>Proof</strong> Let $0 \le s \le t$, thus we have that, \(\begin{align*} \mathbb{E}[Z(t) | \mathcal{F}(s)] &amp;= \mathbb{E} \left[\exp \left\{\sigma W(t) - \frac{1}{2}\sigma^2 t \right\} | \mathcal{F}(s) \right]\\ &amp;= \mathbb{E} \left[\exp \{\sigma (W(t) - W(s))\} \cdot \exp \left\{\sigma W(s) - \frac{1}{2} \sigma^2 t \right\} | \mathcal{F}(s) \right]\\ &amp;= \exp \left\{\sigma W(s) - \frac{1}{2 \sigma^2 t} \right\} \cdot \mathbb{E}[\exp \{\sigma (W(t) - W(s))\} | \mathcal{F}(s)]\\ &amp;= \exp \left\{\sigma W(s) - \frac{1}{2}\sigma^2 s \right\} = Z(s) \end{align*}\) Given by the fact that we can take out what is known and independence to write the last line.</p> <p><strong>First Passage Time</strong> Define the first passage time to level $m$ as, \(\tau_m = \min \{t \ge 0; W(t) = m\}\) which is the first time the BM $W$ reaches the level $m$. If the BM never reaches level $m$, then we set $\tau_m = \infty$. A martingale stopped at a stopping time is still a martingale and thus must have constant expectation. \(1 = Z(0) = \mathbb{E}Z(t \wedge \tau_m) = \mathbb{E} \left[\exp \left\{\sigma W(t \wedge \tau_m) - \frac{1}{2}\sigma^2 (t \wedge \tau_m) \right\} \right]\) where $t \wedge \tau_m$ represents the minimum of $t$ and $\tau_m$. Thus, we assume that $\sigma &gt; 0$ and $m &gt; 0$, the BM is always at or below the level $m$ for $\tau \le \tau_m$ so $0 \le \exp {\sigma W(t \wedge \tau_m} \le e^{\sigma m}$. The extensive proof of the following theorem is redacted.</p> <p><strong>Theorem</strong> The first passage time of a BM to level $m$ is finite almost surely, and the LaPlace transform of its distribution is given by the following, \(\mathbb{E} e^{-\alpha \tau_m} = e^{- |m| \sqrt{2 \alpha}} \quad \text{for all } \alpha &gt;0\) ————————————————————————</p> <p><strong>Reflection Principle</strong> Fix positive level $m$ and positive time $t$, and want to count the BM paths that reach $m$ at or before $t$. There are two such paths,</p> <ol> <li>reach level $m$ prior to $t$ but at time $t$ are at some level $w$ below $m$</li> <li>exceed level $m$ at time $t$ Other cases include ones that are exactly at level $m$ but is unlikely in the case of the Brownian motion and thus has probability 0.</li> </ol> <p>We construct a reflect by switching the up and down moves of the Brownian motion from time $\tau_m$ onwards. The probability that a Brownian Motion ends at exactly $w$ or at exactly $2m - w$ is zero. Consider the paths that reach level $m$ prior to $t$ and are at or below level $w$ at time $t$. This leads to the key reflection equality given by, \(\mathbb{P}\{\tau_m \le t, W(t) \le m\} = \mathbb{P}\{W(t) \ge 2m - w\},\quad w \le m, m &gt; 0\) <strong>Theorem: Random Variable $\tau_m$</strong> The random variable $\tau_m$ has cumulative distribution function given by, \(\mathbb{P}\{\tau_m \le t\} = \frac{2}{\sqrt{2 \pi}} \int_{\frac{|m|}{\sqrt{t}}}^\infty e^{-\frac{y^2}{2}} dy,\quad t \ge 0\) and density given by, \(f_{\tau_m}(t) = \frac{d}{dt} \mathbb{P}\{\tau_m \le t\} = \frac{|m|}{t \sqrt{2 \pi t}} e^{-\frac{m^2}{2t}}, \quad t \ge 0\) <strong>Maximum Date of Brownian Motion</strong> The maximum to date for BM to be: \(M(t) = \max_{0 \le s \le t} W(s)\) which is used in pricing barrier options. For the value of $t$ the random variable $M(t)$ is indicated. For positive $m$ we have that $M(t) \ge m$ if and only if $\tau_m \le t$. \(\mathbb{P}\{M(t) \ge m, W(t) \le w\} = \mathbb{P}\{W(t) \ge 2m - w\},\quad w \le m, m &gt; 0\) which can be written as the joint distribution of $W(t)$ and $M(t)$.</p> <p><strong>Theorem</strong> For $t &gt; 0$ the joint density of $(M(t), W(t))$ is given by the following, \(f_{M(t), W(t)} (m ,w) = \frac{2(2m - w)}{t \sqrt{2 \pi t}} \exp \left\{-\frac{(2m - w)^2}{2t} \right\},\quad w \le m, m &gt; 0\)</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Kenneth Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],tags:"ams"},svg:{scale:1.1,minScale:.8,matchFontHeight:!1}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-notes",title:"notes",description:"Personal-study notes for quant-finance related learning materials",section:"Navigation",handler:()=>{window.location.href="/notes/"}},{id:"notes-stochastic-calculus-for-finance-i",title:"Stochastic Calculus for Finance I",description:"My personal notes for Shreve&#39;s Stochastic Calculus for Finance I",section:"Notes",handler:()=>{window.location.href="/notes/course-1/"}},{id:"notes-stochastic-calculus-for-finance-ii",title:"Stochastic Calculus for Finance II",description:"My personal notes for Shreve&#39;s Stochastic Calculus for Finance II",section:"Notes",handler:()=>{window.location.href="/notes/course-2/"}},{id:"notes-chapter-1-no-arbitrage-pricing-model",title:"Chapter 1 - No-Arbitrage Pricing Model",description:"Notes on the No-Arbitrage Pricing Model.",section:"Notes",handler:()=>{window.location.href="/notes/course-1/chapter-1-no-arbitrage/"}},{id:"notes-chapter-1-exercises",title:"Chapter 1 Exercises",description:"Exercises for Chapter 1 - No-Arbitrage Pricing Model.",section:"Notes",handler:()=>{window.location.href="/notes/course-1/chapter-1-exercises/"}},{id:"notes-chapter-2-exercises",title:"Chapter 2 Exercises",description:"Exercises for Chapter 2 - Probability Theory on Coin Toss Space.",section:"Notes",handler:()=>{window.location.href="/notes/course-1/chapter-2-exercises/"}},{id:"notes-chapter-2-probability-theory-on-coin-toss-space",title:"Chapter 2 - Probability Theory on Coin Toss Space",description:"Notes on probability theory applied to coin toss spaces.",section:"Notes",handler:()=>{window.location.href="/notes/course-1/chapter-2-probability/"}},{id:"notes-chapter-3-state-prices",title:"Chapter 3 - State Prices",description:"Notes on State Prices in financial models.",section:"Notes",handler:()=>{window.location.href="/notes/course-1/chapter-3-state-prices/"}},{id:"notes-chapter-3-exercises",title:"Chapter 3 Exercises",description:"Exercises for Chapter 3 - State Prices.",section:"Notes",handler:()=>{window.location.href="/notes/course-1/chapter-3-exercises/"}},{id:"notes-chapter-4-american-derivative-securities",title:"Chapter 4 - American Derivative Securities",description:"Notes on American derivative securities and their valuation.",section:"Notes",handler:()=>{window.location.href="/notes/course-1/chapter-4-american-derivatives/"}},{id:"notes-chapter-1-general-probability-theory",title:"Chapter 1 - General Probability Theory",description:"Notes on general probability theory and foundational concepts.",section:"Notes",handler:()=>{window.location.href="/notes/course-2/chapter-1-general-probability/"}},{id:"notes-chapter-1-probability-exercises",title:"Chapter 1 - Probability Exercises",description:"Exercises for Chapter 1 - General Probability Theory.",section:"Notes",handler:()=>{window.location.href="/notes/course-2/chapter-1-probability-exercises/"}},{id:"notes-chapter-2-information-exercises",title:"Chapter 2 - Information Exercises",description:"Exercises for Chapter 2 - Information and Conditioning.",section:"Notes",handler:()=>{window.location.href="/notes/course-2/chapter-2-information-exercises/"}},{id:"notes-chapter-2-information-and-conditioning",title:"Chapter 2 - Information and Conditioning",description:"Notes on information theory and conditional probability.",section:"Notes",handler:()=>{window.location.href="/notes/course-2/chapter-2-information-conditioning/"}},{id:"notes-chapter-3-brownian-motion-exercises",title:"Chapter 3 - Brownian Motion Exercises",description:"Exercises for Chapter 3 - Brownian Motion.",section:"Notes",handler:()=>{window.location.href="/notes/course-2/chapter-3-brownian-motion-exercises/"}},{id:"notes-chapter-3-brownian-motion",title:"Chapter 3 - Brownian Motion",description:"Notes on Brownian motion and its applications.",section:"Notes",handler:()=>{window.location.href="/notes/course-2/chapter-3-brownian-motion/"}},{id:"notes-chapter-4-stochastic-calculus-exercises",title:"Chapter 4 - Stochastic Calculus Exercises",description:"Notes on Stochastic Calculus and its applications.",section:"Notes",handler:()=>{window.location.href="/notes/course-2/chapter-4-stochastic-calculus-exercises/"}},{id:"notes-chapter-4-stochastic-calculus",title:"Chapter 4 - Stochastic Calculus",description:"Notes on Brownian motion and its applications.",section:"Notes",handler:()=>{window.location.href="/notes/course-2/chapter-4-stochastic-calculus/"}},{id:"notes-chapter-4-partial-differential-equations-exercises",title:"Chapter 4 - Partial Differential Equations Exercises",description:"Notes on Partial Differential Equations and its applications.",section:"Notes",handler:()=>{window.location.href="/notes/course-2/chapter-4-PDEs-exercises/"}},{id:"notes-chapter-4-partial-differential-equations",title:"Chapter 4 - Partial Differential Equations",description:"Notes on Partial Differential Equations and its applications.",section:"Notes",handler:()=>{window.location.href="/notes/course-2/chapter-4-PDEs/"}},{id:"projects-the-lagging-indicator-chronicles-a-tale-of-policies-playing-catch-up",title:"The Lagging Indicator Chronicles \u2013 A Tale of Policies Playing Catch-Up",description:"Lag time between state-level policy interventions and change points in COVID-19 outcomes in the United States. Read more at ![this article](https://www.cell.com/patterns/fulltext/S2666-3899(21)00149-5).",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-pm2-5-and-the-great-regulatory-vanishing-act-when-pollution-took-a-free-pass",title:"PM2.5 and the Great Regulatory Vanishing Act - When Pollution Took a Free...",description:"An introduction to the statistical models used to assess the impact of EPA rollbacks on air quality in California. https://enveurope.springeropen.com/articles/10.1186/s12302-021-00489-9",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%6F%75@%65%78%61%6D%70%6C%65.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> </body> </html>